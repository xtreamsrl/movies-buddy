{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In the previous section of this workshop, we discussed the importance of embedding a language's semantics in vectors (typically with hundreds of dimensions). These vector representations are not simply meant to be used as inputs to train deep learning models: there are multiple, key applications in search and recommendation without much machine learning - just similarity search. For example, Spotify heavily uses [vector search](https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/) to power its features.\n",
    "\n",
    "After ChatGPT release, practitioners realised they could combine the natural language capabilities of LLMs to manipulate the results of vector search to enhance your queries. The bottleneck, in the end, is always speed: that's why vector databases are so popular. What are they, why are they necessary, and how do they work? Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"movie_buddy\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/movies-buddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Comparing Vectors Using Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "One of the main advantages of having text represented as a vector is that we can compare them, such as by measuring their distance relative to each other or any other new element. In this way, we can retrieve the most similar results!\n",
    "\n",
    "Using different distance metrics, we can calculate how much two points in a multidimensional space are close to each other. The most commonly used in the context of NLP are: \n",
    "\n",
    "- **Cosine similarity**: measures the angle between vectors, so it's about direction, not magnitude.\n",
    "- **Dot product**: measures both the angle and the magnitude of vectors.\n",
    "- **Euclidean distance**: measures the straight-line distance between points.\n",
    "- **Manhattan distance**: measures the distance between points along a grid, like city blocks.\n",
    "\n",
    "Each one is useful in different situations, depending on what aspects of the data are most important for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "![image.png](https://weaviate.io/assets/images/hero-183a22407b0eaf83e53d574aee0a049a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here's another mathematical formulation you might see around for distance metrics:\n",
    "\n",
    "<center>\n",
    "\n",
    "name | formula\n",
    "-----|--------\n",
    "Cosine Similarity | $$d(a, b) = \\cos(\\theta) = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} b_i^2}}$$\n",
    "Dot Product | $$d(a, b) = a \\cdot b = \\sum_{i=1}^{n} a_i b_i$$\n",
    "Euclidean Distance | $$d\\left( a,b\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( a_{i}-b_{i}\\right)^2 }$$\n",
    "Manhattan Distance | $$d(a, b) = \\sum_{i=1}^{n}abs{a_i - b_i}$$\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Distance Metrics Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Not every measurement can become a distance metric. To invent your own, it should satisfy some mathematical properties.\n",
    "\n",
    "Given two vectors $a$ and $b$, a distance metric $d$ must be:\n",
    "\n",
    "1. **Non-negative**: $d(a, b) >= 0$;\n",
    "\n",
    "2. **Symmetric**: $d(a, b) = d(a, b)$;\n",
    "\n",
    "3. **Respect triangle inequality**: $d(a, b) <= d(a, r) + d(r, b)$ for all vectors $a$, $b$, $r$;\n",
    "\n",
    "4. $d(p, q)=0$ only if $p=q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Nearest Neighbour Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "\n",
    "vector_a = np.array([0.9, 0.1, 0.23, 0.15])\n",
    "vector_b = np.array([0.9, 0.30, 0.23, 0.25])\n",
    "\n",
    "cosine_distance = cosine(vector_a, vector_b)\n",
    "dot_distance = np.dot(vector_a, vector_b)\n",
    "euclidean_dist = euclidean(vector_a, vector_b)\n",
    "manhattan_dist = cityblock(vector_a, vector_b)\n",
    "\n",
    "print(\n",
    "    f\"Cosine: {cosine_distance}\\nDot: {dot_distance}\\nEuclidean: {euclidean_dist}\\nManhattan: {manhattan_dist}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Nearest Neighbour Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we have a way to represent pieces of text as vectors, and distance metrics to compare them. The last piece to make an application out of these components is a way to search for the most similar vectors.\n",
    "\n",
    "Naive (or brute force) **Nearest Neighbour Search** is quite trivial to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_closest_vectors(\n",
    "    target_vec: np.ndarray, vectors: np.ndarray, k: int\n",
    ") -> np.ndarray:\n",
    "    distances = np.sqrt(np.sum((vectors - target_vec) ** 2, axis=1))\n",
    "    k_closest_indices = np.argsort(distances)[:k]\n",
    "    closest_vectors = vectors[k_closest_indices]\n",
    "    return closest_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's forget for a moment of real sentences and generate a bunch of random vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIMENSIONS = 500\n",
    "N_VECTORS = 500_000\n",
    "K = 3\n",
    "\n",
    "vectors = np.random.uniform(low=0.0, high=1, size=(N_VECTORS, VECTOR_DIMENSIONS))\n",
    "\n",
    "# the vector to search\n",
    "query_vector = np.random.uniform(low=-1, high=1, size=(VECTOR_DIMENSIONS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Let's see how our native approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s_time = time.time()\n",
    "closest_vectors = k_closest_vectors(query_vector, vectors, K)\n",
    "nn_time = time.time() - s_time\n",
    "\n",
    "f\"To get the closer {K} vectors with naive K-Nearest-Neighbour algorithm took {nn_time:.2} sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Try increasing the N_VECTORS parameter. How long does this take? It's easy to see how and why this approach does not scale: K-Nearest-Neighbour is a $O(N)$ algorithm.\n",
    "\n",
    "An alternative approach is to trade some accuracy for some speed, with Approximate Nearest Neighbours (ANN). There are a lot of ANN algorithms:\n",
    "\n",
    "### HNSW: Hierarchical Navigable Small Worlds\n",
    "**Type**: Graph-based\n",
    "**How It Works**: Imagine navigating a city not by walking through every street but by using shortcuts and hierarchies, like jumping from neighborhood to neighborhood before drilling down to specific streets. HNSW constructs a multi-layered graph where each layer is a more coarse-grained approximation of the data. Searches start at the top layer using these broad shortcuts, then move down to increasingly finer details, making the search efficient even in very large datasets. It's akin to starting from a bird's-eye view of the city and zooming in step-by-step to your destination.\n",
    "\n",
    "* [How it works](https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37)\n",
    "* [Original paper](https://arxiv.org/abs/1603.09320)\n",
    "\n",
    "![image.png](https://weaviate.io/assets/images/ann-hnsw-400840ab1a28768e09a1f4c38e3b4623.png)\n",
    "\n",
    "### ANNOY: Approximate Nearest Neighbor Oh Yeah\n",
    "**Type**: Tree-based\n",
    "**How It Works**: Think of ANNOY as organizing data points into a forest of binary trees, where each tree is constructed by repeatedly choosing two points at random and dividing the space into two partitions based on which side of the line (defined by these points) other points fall. This process creates a set of trees where similar items are likely to end up in the same leaf or nearby leaves. When querying, ANNOY traverses these trees, quickly narrowing down the areas where the nearest neighbors are likely to be found. It's like having a map where similar things are grouped together, and finding what you want becomes faster because you know which area to look in.\n",
    "\n",
    "* [How it works](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html)\n",
    "* Curious about code? Repo [here](https://github.com/spotify/annoy)\n",
    "\n",
    "![image.png](https://weaviate.io/assets/images/ann-annoy-249fabd41b0eebb40ee135b9733f711f.png)\n",
    "\n",
    "### LSH: Locality Sensitive Hashing\n",
    "**Type**: Hash-based\n",
    "**How It Works**: Imagine trying to find documents with similar topics in a huge library. Instead of reading through each, you could assign a unique stamp to each document based on its main themes. Documents with similar themes would have similar stamps, making it easier to find related documents just by comparing stamps. LSH works by transforming data points into a hash code such that similar items are more likely to be hashed to the same \"bucket\" or a nearby bucket. During a search, only items in the same or adjacent buckets are compared, significantly speeding up the search process for similar items. Itâ€™s a way of organizing items so that one can quickly find items in the same category without exhaustive search.\n",
    "\n",
    "* [How it works](https://www.notion.so/Vector-Database-101-1a1926611dc548c1a01a8f939a9dc42c?pvs=21)\n",
    "\n",
    "For the rest of our examples, we will use [Qdrant](https://qdrant.tech/) to build our LLM-powered application. Qdrant is a popular open source vector database. It implements HNSW and can both be used as an in-memory database or as a disk-based one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Get started with Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's try to load our random vectors inside a qdrant collection.\n",
    "\n",
    "After installing qdrant-client, we need to create an instance of the client. Then, we create a \"collection\", i.e. an equivalent of a database table. A collection is a named set of points (vectors with a payload) among which you can search. The vector of each point within the same collection must have the same dimensionality and be compared by a single metric. Vectors can also have a payload, i.e. metadata attached to them.\n",
    "\n",
    "A collection needs to be created with a name and a vector configuration. The vector configuration includes the **vector size** and the **distance metric**. This are fixed, and cannot be changed. Only vectors of the same size can be added to the same collection. The distance metrics will be used to perform the approximate search. The choice of the distance metrics also depends on the method of neural network encoder training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "\n",
    "COLLECTION_NAME = \"random_vectors\"\n",
    "DISTANCE = models.Distance.COSINE\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(size=VECTOR_DIMENSIONS, distance=DISTANCE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Here we created a very basic collection. Qdrant's [documentation](https://qdrant.tech/documentation/concepts/collections) explores all of their capabilities, such as how to configure some of the optimisation they can perform. Qdrant can store sparse vectors or quantize them to increase performance.\n",
    "\n",
    "Here we upload our vectors to the collection with the `upload_collection` method. This performs batch uploads. The operation is slow, since the database needs to build the index structure to subsequently perform the ANN search. By default, qdrant creates an index when there are more than 20_000 records (below said threshold, brute force search is used). We can control this threshold with the `indexing_threshold` parameter, as explained [here](https://qdrant.tech/documentation/tutorials/bulk-upload/#disable-indexing-during-upload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.upload_collection(collection_name=COLLECTION_NAME, vectors=vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can retrieve some information about our collection in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.get_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "However, keep in mind that the number of vectors is an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "qdrant.search(collection_name=COLLECTION_NAME, query_vector=query_vector, limit=K)\n",
    "ann_time = time.time() - s_time\n",
    "\n",
    "f\"To get the closest {K} vectors with a vector database took {ann_time:.2} sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## What About Movies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "You should now be and expert in embeddings, nearest-neighbours search and vector databases such as qdrant. How about you try yourself to import our movies dataset in a new collection? To avoid re-compunting the embeddings, you can use the `get_embeddings` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.data import get_embeddings, get_movies_dataset\n",
    "\n",
    "movies, movies_embeddings = get_movies_dataset(), get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Build the Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "As a first step, you still need to import the encoding model. Its size will be used to determine the size of the collection. Use the same model as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Create a `QdrantClient` in-memory instance. Create a collection named `movies`.\n",
    "\n",
    "To determine the vector configuration, use the `get_sentence_embedding_dimension` method of the encoder. Use cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = ...\n",
    "\n",
    "COLLECTION_NAME = ...\n",
    "\n",
    "# (re)create the collection named `COLLECTION_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "The smallest instance of Qdrant is a `Record`. The vector contains an index number, a record and a payload. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = models.Record(\n",
    "    id=1,\n",
    "    vector=movies_embeddings[0],\n",
    "    payload={\n",
    "        \"title\": movies.iloc[0][\"title\"],\n",
    "        \"overview\": movies.iloc[0][\"overview\"],\n",
    "        \"release_date\": movies.iloc[0][\"release_date\"],\n",
    "        \"runtime\": movies.iloc[0][\"runtime\"],\n",
    "        \"genre\": movies.iloc[0][\"genre\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "It's your turn. Create a list of records that will be uploaded to the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "To upload the records, use `qdrant.upload_points`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Now, you can have fun searching your prefered movies!\n",
    "\n",
    "Just define a `query` variable, encode it and use `qdrant.search`. This will return a list of hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ...\n",
    "encoded_prompt = ...\n",
    "\n",
    "results = qdrant.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    payload = result.payload\n",
    "    print(\n",
    "        \"Title: {title}\\nRelease date: {release_date}\\nRuntime: {runtime}\\n\\n\".format(\n",
    "            title=payload[\"title\"],\n",
    "            release_date=payload[\"release_date\"],\n",
    "            runtime=payload[\"runtime\"],\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
